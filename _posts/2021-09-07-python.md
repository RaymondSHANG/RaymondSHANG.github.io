---
layout: post
title: "”Python Google Scholar"
subtitle: "Web Scraping with Selenium"
date: 2021-09-07 14:52:50
header-style: text
catalog: true
author: "Yuan"
tags: 
    - Google Scholar
    - Web Scaping
    - Python
    - selenium
    - BeautifulSoup
    - argparse
---

> Life is hard, ignorance makes it harder

I was trying to analyze my google citations. After testing several github repositories, I found two of them are useful for my task.

One is [gscholar-citations-crawler](https://github.com/thu-pacman/gscholar-citations-crawler), the code is simple, including all nessisary elements for a basic webpage scraping task, including parameters input (argparse), BeautifulSoup html parsing, result output. However, it will fail after several tries because it failed to deal with the "CAPTCHA" problem. Also, the original code failed to catch citation ids and thus failed to save them into bib format.

Another repository called [etudier](https://github.com/edsu/etudier) bypass this blocking problem using selenium. Also, it saved all citation info into json format for post processing. For my own task, I need to set the search depth to "0", so only first level of citations were including. To use etudier, I need to extract all my publications into a list, and use each element of that list as an input to etudier. My final code for this task is posted in my github(https://github.com/RaymondSHANG?tab=repositories).

#### Other useful references:
1. Dimitry Zub wrote an awesome blog about 'Scrape Google Scholar with Python'(https://dev.to/dimitryzub/scrape-google-scholar-with-python-32oh), which has dicussed almost all bugs/problems I met.
https://dev.to/dimitryzub/scrape-google-scholar-with-python-32oh

2. **SerpApi**(https://serpapi.com/) is another cool tool to bypass the blocking problems. We could register an account to get an api_key for free. It's fast and coding is efficient. The only thing is probably you need to learn how to use SerApi.
https://serpapi.com/


---
